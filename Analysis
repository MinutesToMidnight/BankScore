# Предобработка
Для численных значений - StandartScaler
Для категориальных - OneHotEncoder
Бинарные данные передаются без преобразований
Ввиду небольшого количества и разнообразия данных в категориальных данных можно использовать OneHotEncoder. Так модели будет легче
присваивать весовые коэффициенты, посколько при кодировке данных иным образом существует риск создать такую кодировку, что модель
не сможет численно подобрать соответствующие весовые коэффициенты, способные выделять зависимости данных.

# Выбор моделей
LinearRegressor как самый простой пример
ExtraTrees, GradientBoosting, AdaBoost, SVR
Таким образом, имеем модели с различным подходом к решению задачи - градиентный бустинг, бустинг, метод векторов

# Оценка метрик
Linear Regressor
mean_absolute_error: 24.534075799132605
r2_score: 0.6232015217398985
mean_absolute_percentage_error: 0.03433560976853434

ExtraTrees
mean_absolute_error: 24.538186264825416
r2_score: 0.6231711204607673
mean_absolute_percentage_error: 0.034339815663000046

AdaBoost
mean_absolute_error: 24.638562231213562
r2_score: 0.6229129805569231
mean_absolute_percentage_error: 0.03449327500630359

SVR
mean_absolute_error: 24.160484168728978
r2_score: 0.6105571637841244
mean_absolute_percentage_error: 0.033740570576943144

GBR
mean_absolute_error: 24.239279996716835
r2_score: 0.6197576097082611
mean_absolute_percentage_error: 0.03391996933205588

Voting Regressor
mean_absolute_error: 24.305986165233044
r2_score: 0.621990983888623
mean_absolute_percentage_error: 0.03399992272296711

Метрики моделей при подобранных гиперпараметрах отличаются незначительно. Более того, линейная регрессия не уступает по метрикам более сложным моделям,
что говорит о наличии несложных зависимостях между данными и целевым значением. В качестве основной модели можно остановиться на линейной регрессии.
Значения в целевой переменной:
Максимальное - 850
Минимальное - 530
Медиана - 715
Среднее отклонение - 49.74
Диапазон значений - 320
Абсолютная ошибка у всех моделей прмерно равно 24.3
Это значительно ниже среднего отклонения и много меньше всего диапазона значений, что говорит об относительно хороших результатах.
Абсолютная процентная ошибка составляет 3%, что означает: в среднем предсказания модели отличались на 3 процента от настоящих данных.
r2 показыаает коэффициент детерминации модели. Метрика отображает, насколько хорошо модель объясняет изменчивость данных.
Попробуем улучшить этот параметр используя линейную регрессию и различные техники по предобработке данных.

# Анализ и изменение признаков
Исключим аномалии при помощи z-score и оценим показатели модели. Добавим фиксированное значение в random_state. Так данные будут всегда разбиваться одинаково.

Без обработки:
mean_absolute_error: 25.191065531633534
r2_score: 0.6188774194603408
mean_absolute_percentage_error: 0.0355160021331868

Исключение аномалий:
df = df[(np.abs(stats.zscore(df[numeric])) < 3).all(axis=1)] (z_score < 3)
mean_absolute_error: 25.191065531633534
r2_score: 0.6188774194603408
mean_absolute_percentage_error: 0.0355160021331868

df = df[(np.abs(stats.zscore(df[numeric])) < 2.5).all(axis=1)] (z_score < 2.5)
mean_absolute_error: 25.191065531633534
r2_score: 0.6188774194603408
mean_absolute_percentage_error: 0.0355160021331868

Исключение аномалий никак не повлияло на результат.

Сформируем новые признаки на основе имеющихся.
poly = PolynomialFeatures(degree=2, interaction_only=True)

Добавление полниномиальных признаков:
degree = 2
mean_absolute_error: 25.801502629034154
r2_score: 0.60047103892477
mean_absolute_percentage_error: 0.03637604417310727

degree = 3
mean_absolute_error: 33.15462096516986
r2_score: 0.28279176476883483
mean_absolute_percentage_error: 0.04672262931595994

Добавление полиномиальных признаков вносит шум и ухудшаeт метрики модели. Время выполнения значительно увеличивается.

Выделим важнейшие признаки.
selector = SelectKBest(score_func=f_regression, k=10)

k = 5
mean_absolute_error: 25.203440838161537
r2_score: 0.6187976229667694
mean_absolute_percentage_error: 0.03553542024837231

k = 10
mean_absolute_error: 25.205177256921594
r2_score: 0.6185697860283541
mean_absolute_percentage_error: 0.03553667769119078

k = 20
Linear Regressor
mean_absolute_error: 25.195222805092406
r2_score: 0.6186662499070011
mean_absolute_percentage_error: 0.03551981545055752

Сокращение признакового пространства почти не влияет на метрики модели.

Откуда делаем вывод, что для повышения детерминации модели необходимо добавление новых независимых признаков.
Вышеперечисленные операции проделаны и для других регрессоров. Результат не изменяется. Для наглядности оставлена только линейная регрессия

# Оценка времени выполнения
Время предобработки: 0.05539709999720799 s
Предобработка, как и ожидается, не требует особых затрат по времени.

Время подбора гиперпараметров ExtraTrees: 721.7934969999988 s
Время подбора гиперпараметров AdaBoost: 1686.6049452000007 s
Время подбора гиперпараметров SVR: 2783.3431200999985 s
Время подбора гиперпараметров GBR: 3567.722231599997 s
Общее время подбора гиперпараметров: 8759.463793899995 s

Можем оценить скорость работы разных алгоритмов. Это исключительно приблизтельные значения, поскольку в каждом из пространств моделей определено разное количество гиперпараметров
и различные диапозоны. ExtraTrees имел гиперпараметр n_jobs = -1, который указывает программе на возможность использования ядер и мощностей ЦП, что позволило значительно ускорить
работу алгоритма, несмотря на его сложность.

Время обучения ExtraTrees: 0.3588668000011239 s
Время предсказания ExtraTrees: 0.07230359999812208 s
Время обучения AdaBoost: 3.3545523999928264 s
Время предсказания AdaBoost: 0.3078113000083249 s
Время обучения SVR: 10.959614199993666 s
Время предсказания SVR: 2.977110099993297 s
Время обучения GBR: 0.18411840000771917 s
Время предсказания GBR: 0.0028144000098109245 s
Общее время обучения: 14.857151799995336 s
Общее время предсказания: 3.360039400009555 s

Обучение занимает гораздо больше времени. Время обучения и предсказния GBR самое непродолжительное вопреки времени подбора гиперпаметров для этой модели.
